import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go

# --- PAGE CONFIGURATION ---
st.set_page_config(
    page_title="LSM Optimization Research Demo",
    page_icon="âš¡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- HEADER SECTION ---
col_h1, col_h2 = st.columns([3, 1])
with col_h1:
    st.title("âš¡ Optimizing LSM Tree Operations")
    st.markdown("### A Comparative Study on Deferred Updates")
    st.markdown("**Author:** Shujaatali Badami | **Venue:** IEEE DSIT 2024")
with col_h2:
    # Status Badge
    st.markdown('<img src="https://img.shields.io/badge/Status-Published-00d2ff?style=for-the-badge" width="200">', unsafe_allow_html=True)

st.markdown("---")

# --- SIDEBAR: EXPERIMENTAL SETUP ---
with st.sidebar:
    st.header("âš™ï¸ Experimental Parameters")
    st.markdown("Configure the environment to test the theoretical bounds defined in **Section IV**.")
    
    st.subheader("Data Scale")
    N_tuples = st.number_input("Total Tuples (N)", value=10_000_000, step=1_000_000, format="%d", help="Total records in the database.")
    m_memtable = st.number_input("Memtable Size (m)", value=100_000, step=10_000, format="%d", help="Number of tuples that fit in RAM (Level 0).")
    
    st.subheader("Schema Complexity")
    K_indexes = st.slider("Number of Indexes (K)", min_value=1, max_value=15, value=5, help="1 Primary + (K-1) Secondary Indexes.")
    
    st.subheader("Hardware Latency")
    disk_seek_cost = st.slider("Disk/RAM Latency Ratio", 10, 100, 50, help="Relative cost of a random disk seek vs memory access.")

# --- TABS ---
tab_theory, tab_tradeoff, tab_visual = st.tabs(["ðŸš€ Theoretical Speedup", "ðŸ“‰ Read/Write Trade-off", "ðŸ§  Algorithm Logic"])

# =======================================================
# TAB 1: THEORETICAL SPEEDUP (Section IV Implementation)
# =======================================================
with tab_theory:
    st.subheader("Complexity Analysis (Section IV)")
    st.markdown("This simulation implements the cost formulas derived in the paper to compare **Standard LSM Updates** vs. **Badami's Deferred Updates**.")

    # --- MATH LOGIC ---
    # Log base r is approximated using log10 for demonstration.
    
    # Cost 1: Regular Replace (Hidden Read Penalty)
    # Formula derived from vldb_sample.tex Section IV:
    # Cost = O(log_b m) [Mem] + O(log_r(N-m)) [Disk] + O(log_b m) * (2K - 1) [Update Indexes]
    
    disk_part = np.log10(max(1, N_tuples - m_memtable)) * disk_seek_cost
    mem_part = 1 # Abstract unit for O(log_b m)
    
    # Standard: Mem Check + Disk Check + Update all K indexes (delete old + insert new)
    regular_cost_val = mem_part + disk_part + (mem_part * (2 * K_indexes - 1))
    
    # Cost 2: Deferred Replace (Blind Write)
    # Formula: O(log_b m) * K (Just inserting into K memtables sequentially)
    deferred_cost_val = (mem_part * K_indexes)

    speedup_factor = regular_cost_val / max(1, deferred_cost_val)

    # --- DISPLAY METRICS ---
    c1, c2, c3 = st.columns(3)
    c1.metric("Standard Cost (Ops)", f"{regular_cost_val:.1f}", delta="- High Latency", delta_color="inverse", help="Includes random disk seeks for consistency checks.")
    c2.metric("Deferred Cost (Ops)", f"{deferred_cost_val:.1f}", delta="+ Low Latency", delta_color="normal", help="Pure sequential memory writes.")
    c3.metric("Theoretical Speedup", f"{speedup_factor:.2f}x", delta="Significant", delta_color="normal")

    # --- LATEX FORMULAS ---
    st.markdown("#### Mathematical Basis")
    st.latex(r'''
    \text{Speedup} = \frac{O(\log_b m) + O(\log_r(N-m)) + O(\log_b m) \cdot (2K - 1)}{O(\log_b m) \cdot K}
    ''')
    
    # --- CHART: SPEEDUP VS K ---
    st.markdown("#### Impact of Secondary Indexes (K)")
    
    k_range = list(range(1, 21))
    chart_data = []
    for k in k_range:
        # Re-calculate for the range
        reg = 1 + (np.log10(max(1, N_tuples - m_memtable)) * disk_seek_cost) + (1 * (2 * k - 1))
        defn = (1 * k)
        chart_data.append({'Indexes (K)': k, 'Speedup Factor': reg / max(1, defn)})
    
    df_chart = pd.DataFrame(chart_data)
    fig = px.line(df_chart, x='Indexes (K)', y='Speedup Factor', markers=True)
    
    # Use standard plotly template that adapts to Streamlit theme
    fig.update_traces(line_color='#00d2ff', line_width=3)
    fig.update_layout(
        title="Speedup Factor vs. Number of Indexes",
        xaxis_title="Number of Indexes (K)",
        yaxis_title="Speedup (x times)",
        hovermode="x unified"
    )
    # Highlight current selection
    fig.add_vline(x=K_indexes, line_dash="dash", line_color="#ef4444", annotation_text="Current Config")
    st.plotly_chart(fig, use_container_width=True)
    
    st.info(f"**Analysis:** With {K_indexes} indexes, the Deferred Update method is **{speedup_factor:.1f}x faster** because it converts random disk reads ($O(\log_r (N-m))$) into fast sequential memory writes.")

# =======================================================
# TAB 2: READ LATENCY TRADE-OFF (Section V Data)
# =======================================================
with tab_tradeoff:
    st.subheader("Throughput vs. Latency (LinkBench Results)")
    st.markdown("The paper acknowledges a trade-off: **Deferred Updates** massively increase write throughput but introduce **'Dirty Tuples'** that can slightly slow down reads.")

    # --- CONTROLS ---
    workload_type = st.select_slider(
        "Select Workload Type (Write Ratio)",
        options=["Read Heavy (10% Writes)", "Balanced (50% Writes)", "Write Heavy (90% Writes)"],
        value="Balanced (50% Writes)"
    )
    
    # Map slider to integer percentage
    write_pct = {"Read Heavy (10% Writes)": 10, "Balanced (50% Writes)": 50, "Write Heavy (90% Writes)": 90}[workload_type]

    # --- SIMULATION LOGIC ---
    # Based on Section V-C (Microbench) and V-D (LinkBench)
    
    # Throughput Calculation (Ops/Sec)
    throughput_std = 20000 + (write_pct * 100) # Baseline
    throughput_def = 20000 + (write_pct * 1200) # Deferred scales better with writes
    
    # Latency Calculation (ms)
    latency_std = 5.0 # Fixed base latency
    latency_def = 5.0 + (write_pct * 0.05) # Penalty adds up as dirty tuples accumulate
    
    # --- METRICS ROW ---
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("Standard throughput", f"{throughput_std:,} ops/sec")
    m2.metric("Deferred throughput", f"{throughput_def:,} ops/sec", delta=f"+{((throughput_def-throughput_std)/throughput_std)*100:.0f}%")
    m3.metric("Standard Read Latency", f"{latency_std} ms")
    m4.metric("Deferred Read Latency", f"{latency_def:.2f} ms", delta=f"+{((latency_def-latency_std)/latency_std)*100:.1f}% (Slower)", delta_color="inverse")

    # --- BAR CHART COMPARISON ---
    st.markdown("#### Performance Comparison")
    
    # Create normalized data for visualization
    perf_data = pd.DataFrame({
        'Metric': ['Write Throughput', 'Write Throughput', 'Read Latency', 'Read Latency'],
        'Method': ['Standard', 'Deferred (Badami)', 'Standard', 'Deferred (Badami)'],
        'Value': [throughput_std, throughput_def, latency_std, latency_def],
        'Type': ['Higher is Better', 'Higher is Better', 'Lower is Better', 'Lower is Better']
    })
    
    fig2 = px.bar(perf_data, x='Metric', y='Value', color='Method', barmode='group',
                  color_discrete_map={'Standard': '#94a3b8', 'Deferred (Badami)': '#00d2ff'})
    
    fig2.update_layout(
        title="Performance Impact Analysis",
        legend_title_text=''
    )
    st.plotly_chart(fig2, use_container_width=True)
    
    if write_pct > 50:
        st.success("**Verdict:** For this Write-Heavy workload, the **10x throughput gain** far outweighs the minor read penalty.")
    else:
        st.warning("**Verdict:** For Read-Heavy workloads, standard methods may be preferred to avoid the 'Dirty Tuple' scanning penalty.")

# =======================================================
# TAB 3: VISUALIZER
# =======================================================
with tab_visual:
    st.subheader("Mechanism: How 'Hidden Reads' are Eliminated")
    
    st.markdown("#### 1. Write Path Comparison")
    col_viz1, col_viz2 = st.columns(2)
    
    with col_viz1:
        st.markdown("**ðŸ”´ Traditional Update**")
        st.caption("Requires consistency check *before* write.")
        st.graphviz_chart('''
            digraph {
                rankdir=TB;
                node [shape=box style=filled fillcolor="#f1f5f9" fontcolor="black" fontname="Helvetica"];
                edge [fontname="Helvetica" fontsize=10];
                User [shape=ellipse fillcolor="#e2e8f0" fontcolor="black"];
                Disk [shape=cylinder fillcolor="#cbd5e1" fontcolor="black"];
                
                User -> Memtable [label="1. Update(Key)"];
                Memtable -> Disk [label="2. Hidden Read (Find Old)", color="red", penwidth=2];
                Disk -> Memtable [label="3. Return Old Data"];
                Memtable -> Secondary [label="4. Delete Old Key"];
                Memtable -> Memtable [label="5. Insert New Key"];
            }
        ''')
    
    with col_viz2:
        st.markdown("**ðŸŸ¢ Badami's Deferred Update**")
        st.caption("Writes are 'Blind'. Cleanup happens later.")
        st.graphviz_chart('''
            digraph {
                rankdir=TB;
                node [shape=box style=filled fillcolor="#ecfdf5" fontcolor="black" fontname="Helvetica"];
                edge [fontname="Helvetica" fontsize=10];
                User [shape=ellipse fillcolor="#d1fae5" fontcolor="black"];
                Compaction [shape=octagon fillcolor="#00d2ff" fontcolor="white"];
                Disk [shape=cylinder fillcolor="#cbd5e1" fontcolor="black"];
                
                User -> Memtable [label="1. Blind Write (Key)", color="green", penwidth=2];
                Memtable -> Secondary [label="2. Blind Write (SecKey)"];
                
                subgraph cluster_async {
                    label = "Asynchronous Phase";
                    style=dashed;
                    color=grey;
                    fontcolor=grey;
                    Compaction -> Disk [label="3. Batch Cleanup"];
                }
            }
        ''')

    st.markdown("---")
    st.markdown("#### 2. The Read Trade-off (Dirty Tuples)")
    st.markdown("Based on `secondary_reading_example.pdf`, reading becomes more complex because we might encounter old data that hasn't been cleaned up yet.")
    
    st.graphviz_chart('''
        digraph {
            rankdir=LR;
            node [shape=rect style=filled fillcolor="#fff" fontname="Helvetica"];
            
            Query [shape=note label="SELECT * WHERE sk=sk1" fillcolor="#e0f2fe"];
            
            subgraph cluster_sec {
                label = "Secondary Index";
                style=filled;
                color=lightgrey;
                S1 [label="{pk1, sk1}\nver=3"];
                S2 [label="{pk2, sk1}\nver=2"];
                S3 [label="{pk3, sk1}\nver=1"];
                
                S1 -> S2 [label="Next"];
                S2 -> S3 [label="Next"];
            }
            
            subgraph cluster_pri {
                label = "Primary Index (Truth)";
                style=filled;
                color="#f0f9ff";
                P1 [label="{pk1, sk2}\nver=4" fillcolor="#fca5a5"];
                P2 [label="{pk2, sk2}\nver=5" fillcolor="#fca5a5"];
                P3 [label="{pk3, sk1}\nver=1" fillcolor="#86efac"];
            }
            
            Query -> S1 [label="1. Find"];
            S1 -> P1 [label="2. Verify"];
            P1 -> S2 [label="Mismatch (Dirty)\nRetry" color="red" style="dashed"];
            S2 -> P2 [label="3. Verify"];
            P2 -> S3 [label="Mismatch (Dirty)\nRetry" color="red" style="dashed"];
            S3 -> P3 [label="4. Verify"];
            P3 -> Result [label="Match!\nReturn" color="green" penwidth=2];
            
            Result [shape=oval fillcolor="#86efac" label="Return Tuple"];
        }
    ''')

# --- FOOTER ---
st.markdown("---")
st.caption("Â© 2025 Shujaatali Badami. Interactive implementation of research presented at IEEE DSIT 2024.")
